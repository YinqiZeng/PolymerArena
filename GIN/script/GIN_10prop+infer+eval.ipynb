{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0581f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.nn import MLP\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2731af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_result2.0_zero_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c22f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e892fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task groups\n",
    "TARGET_COLS_PHYS_CHEM = [\"Solubility\", \"Molecular_Weight\", \"Log_P\", \"BoilingPoint\"]     # 4\n",
    "TARGET_COLS_OPT_QUANT = [\"sigma_780nm\", \"max_sigma\", \"ISC(S1-T1)\"]                      # 3\n",
    "TARGET_COLS_STRUC_TOX = [\"SAscore\", \"Tox_score\"]                                        # 2\n",
    "TARGET_COL_OTHER      = [\"is_aromatic\"]                                                 # 1\n",
    "\n",
    "REG_COLS = TARGET_COLS_PHYS_CHEM + TARGET_COLS_OPT_QUANT + TARGET_COLS_STRUC_TOX        # 9 regression cols\n",
    "BIN_COL  = TARGET_COL_OTHER[0]                                                          # 1 binary col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "568771cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess SMILES strings to generate graphs using RDKit and PyG\n",
    "def smiles_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(\"Invalid SMILES string\")\n",
    "        return None\n",
    "    # Generate the graph (atom features and bond information)\n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append(atom.GetAtomicNum())\n",
    "    \n",
    "    # Create a bond list (edges)\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_index.append([i, j])\n",
    "        edge_index.append([j, i])  # Because bonds are bidirectional\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)  # Atom features\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "488c6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyG Dataset wrapper\n",
    "class GraphListDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b76ae0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total graphs: 211137\n"
     ]
    }
   ],
   "source": [
    "# create graphs dataset\n",
    "data_list = []\n",
    "for _, row in df.iterrows():\n",
    "    g = smiles_to_graph(row[\"SMILES\"])\n",
    "    if g is None:\n",
    "        continue\n",
    "\n",
    "    y_reg_raw  = row[REG_COLS].to_numpy(dtype=np.float32)              # (9,)\n",
    "    y_arom_raw = np.array([row[BIN_COL]], dtype=np.float32)            # (1,) should be 0/1\n",
    "\n",
    "    g.y_reg_raw  = torch.tensor(y_reg_raw, dtype=torch.float32)        # (9,)\n",
    "    g.y_arom_raw = torch.tensor(y_arom_raw, dtype=torch.float32)       # (1,)\n",
    "\n",
    "    data_list.append(g)\n",
    "\n",
    "dataset = GraphListDataset(data_list)\n",
    "print(f\"Total graphs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a863f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph[0].x shape: (14, 1)\n",
      "graph[0].edge_index shape: (2, 28)\n",
      "graph[0].y_raw shape: (9,)\n",
      "graph[0].y_raw shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# 2) inspect one graph's tensor shapes\n",
    "g0 = dataset[211136]\n",
    "print(\"graph[0].x shape:\", tuple(g0.x.shape))                     # (num_nodes, num_node_features)\n",
    "print(\"graph[0].edge_index shape:\", tuple(g0.edge_index.shape))    # (2, num_edges*2) if bidirectional\n",
    "print(\"graph[0].y_raw shape:\", tuple(g0.y_reg_raw.shape))          # (len(TARGET_COLS),)\n",
    "print(\"graph[0].y_raw shape:\", tuple(g0.y_arom_raw.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb1f6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test datasets (95/5 split)\n",
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "train_cut = int(0.95 * len(dataset))\n",
    "test_size = len(dataset) - train_cut\n",
    "train_dataset = dataset[:train_cut]\n",
    "test_dataset = dataset[train_cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "388ede23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 200580\n",
      "Number of validation graphs: 10557\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of validation graphs: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-graph ZERO mask on the RAW regression targets (REG_COLS)\n",
    "# Attach as d.y_mask\n",
    "for d in train_dataset:\n",
    "    d.y_mask_raw = (d.y_reg_raw != 0).float()   # (9,) 1==valid, 0==zero\n",
    "for d in test_dataset:\n",
    "    d.y_mask_raw = (d.y_reg_raw != 0).float()   # (9,) 1==valid, 0==zero\n",
    "\n",
    "# Fit scaler on TRAIN regression targets\n",
    "scaler = StandardScaler()\n",
    "train_reg = torch.stack([d.y_reg_raw for d in train_dataset], dim=0).cpu().numpy()  # (N_train, 9)\n",
    "scaler.fit(train_reg)\n",
    "\n",
    "# Attach normalized regression targets + raw binary targets\n",
    "for d in train_dataset:\n",
    "    reg_norm = scaler.transform(d.y_reg_raw.view(1, -1).cpu().numpy()).astype(np.float32)[0]  # (9,)\n",
    "    d.y_reg  = torch.tensor(reg_norm, dtype=torch.float32).view(1, -1)                        # (1, 9)\n",
    "    d.y_mask = d.y_mask_raw.view(1, -1)                                                       # (1, 9)\n",
    "    d.y_arom = d.y_arom_raw.view(1, -1).float()                                               # (1, 1) 0/1\n",
    "\n",
    "\n",
    "for d in test_dataset:\n",
    "    reg_norm = scaler.transform(d.y_reg_raw.view(1, -1).cpu().numpy()).astype(np.float32)[0]\n",
    "    d.y_reg  = torch.tensor(reg_norm, dtype=torch.float32).view(1, -1)\n",
    "    d.y_mask = d.y_mask_raw.view(1, -1)\n",
    "    d.y_arom = d.y_arom_raw.view(1, -1).float()\n",
    "\n",
    "# Build loaders AFTER targets are attached\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3441ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_reg shape: (200580, 9)\n",
      "scaler.mean_.shape: (9,)\n",
      "scaler.scale_.shape: (9,)\n",
      "first 9 means: [6.68712438e+04 1.61447319e+02 9.86596597e-01 2.48093641e+02\n",
      " 1.21627415e+02 1.51308171e+02 1.07200107e-01 1.22256239e-01\n",
      " 5.95072797e-01]\n",
      "first 9 stds: [1.58453296e+05 6.90902231e+01 1.49690708e+00 1.41872937e+02\n",
      " 1.00084062e+02 1.15488243e+02 8.75399192e-02 1.20896360e-01\n",
      " 9.47371486e-02]\n"
     ]
    }
   ],
   "source": [
    "# sanity check 1\n",
    "print(\"train_reg shape:\", train_reg.shape)          # should be (N_train, 9)\n",
    "print(\"scaler.mean_.shape:\", scaler.mean_.shape)  # (9,)\n",
    "print(\"scaler.scale_.shape:\", scaler.scale_.shape) # (9,)\n",
    "print(\"first 9 means:\", scaler.mean_[:10])\n",
    "print(\"first 9 stds:\", scaler.scale_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6ef0342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col means (train, after): [-4.2087839e-07 -1.2795178e-08 -2.8530174e-08 -4.3606096e-07\n",
      " -1.0712035e-07 -1.8985979e-07  1.9244770e-07  7.3182322e-09\n",
      " -5.9837544e-09]\n",
      "col stds  (train, after): [0.99959755 1.0002462  0.99999523 0.9999992  0.9998855  1.0001272\n",
      " 1.0000625  0.9999858  0.9999738 ]\n"
     ]
    }
   ],
   "source": [
    "# sanity check 2 \n",
    "train_y_norm = scaler.transform(train_reg)\n",
    "print(\"col means (train, after):\", train_y_norm.mean(axis=0))  # when transform again, mean close to 0, and stds close to 1\n",
    "print(\"col stds  (train, after):\", train_y_norm.std(axis=0))  # which is normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "394a79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskGIN(nn.Module):\n",
    "    def __init__(self, hidden_channels=128):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        # GCN layers\n",
    "        # GINConv layers, each with an MLP\n",
    "        mlp1 = MLP([1, hidden_channels, hidden_channels])\n",
    "        self.conv1 = GINConv(nn=mlp1, train_eps=False)\n",
    "\n",
    "        mlp2 = MLP([hidden_channels, hidden_channels, hidden_channels])\n",
    "        self.conv2 = GINConv(nn=mlp2, train_eps=False)\n",
    "\n",
    "        mlp3 = MLP([hidden_channels, hidden_channels, hidden_channels])\n",
    "        self.conv3 = GINConv(nn=mlp3, train_eps=False)\n",
    "\n",
    "        # Fully connected layers for different tasks (heads)\n",
    "\n",
    "        # For phys_chem head: 128 -> 64 -> 4\n",
    "        self.fc_phys_chem_1 = nn.Linear(128, 64)\n",
    "        self.fc_phys_chem_2 = nn.Linear(64, 4)\n",
    "\n",
    "        # For opt_quant head: 128 -> 64 -> 3\n",
    "        self.fc_opt_quant_1 = nn.Linear(128, 64)\n",
    "        self.fc_opt_quant_2 = nn.Linear(64, 3)\n",
    "\n",
    "        # For struc_tox head: 128 -> 64 -> 2\n",
    "        self.fc_struc_tox_1 = nn.Linear(128, 64)\n",
    "        self.fc_struc_tox_2 = nn.Linear(64, 2)\n",
    "\n",
    "        # For other head: 128 -> 64 -> 32 -> 1 (logits)\n",
    "        self.fc_other_1 = nn.Linear(128, 64)\n",
    "        self.fc_other_2 = nn.Linear(64, 32)\n",
    "        self.fc_other_3 = nn.Linear(32, 1)  # logits for BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # Apply global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        # Phys_chem # (B,4)\n",
    "        phys = self.fc_phys_chem_1(x).relu()\n",
    "        phys = self.fc_phys_chem_2(phys)\n",
    "\n",
    "        # Opt_quant # (B,3)\n",
    "        opt = self.fc_opt_quant_1(x).relu()\n",
    "        opt = self.fc_opt_quant_2(opt)\n",
    "\n",
    "        # Struc_tox # (B,2)\n",
    "        tox = self.fc_struc_tox_1(x).relu()\n",
    "        tox = self.fc_struc_tox_2(tox)\n",
    "\n",
    "        # Other # (B,1) logits\n",
    "        arom = self.fc_other_1(x).relu()\n",
    "        arom = self.fc_other_2(arom).relu()\n",
    "        arom = self.fc_other_3(arom)  # logits for BCEWithLogitsLoss\n",
    "\n",
    "        return phys, opt, tox, arom\n",
    "    \n",
    "\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/compile/gin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1455fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiTaskGIN(\n",
      "  (conv1): GINConv(nn=MLP(1, 128, 128))\n",
      "  (conv2): GINConv(nn=MLP(128, 128, 128))\n",
      "  (conv3): GINConv(nn=MLP(128, 128, 128))\n",
      "  (fc_phys_chem_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_phys_chem_2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (fc_opt_quant_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_opt_quant_2): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (fc_struc_tox_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_struc_tox_2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (fc_other_1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_other_2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc_other_3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskGIN(hidden_channels=128).to(device)\n",
    "\n",
    "# Use AdamW optimizer instead of Adam\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Learning rate scheduler: Reduce learning rate on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5, threshold=1e-4, threshold_mode='rel'\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# UPDATED masked MSE with new semantics:\n",
    "# mask==1 means \"use this entry\"\n",
    "# mask==0 means \"ignore this entry\"\n",
    "# ---------------------------------------------------------\n",
    "def masked_mse(pred: torch.Tensor, target: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    pred/target: (B, k)\n",
    "    valid_mask:  (B, k) with 1==valid/use, 0==ignore\n",
    "    \"\"\"\n",
    "    valid = valid_mask.to(dtype=pred.dtype)\n",
    "    se = (pred - target).pow(2) * valid\n",
    "    denom = valid.sum().clamp_min(1.0)\n",
    "    return se.sum() / denom\n",
    "\n",
    "criterion_bce = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00b179d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        phys_out, opt_out, tox_out, arom_logit = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        y_reg  = data.y_reg          # (B, 7)\n",
    "        y_mask = data.y_mask         # (B, 7)\n",
    "        y_arom = data.y_arom         # (B, 1)\n",
    "\n",
    "        y_phys = y_reg[:, 0:4]\n",
    "        y_opt  = y_reg[:, 4:7]\n",
    "        y_tox  = y_reg[:, 7:9]\n",
    "\n",
    "        m_phys = y_mask[:, 0:4]\n",
    "        m_opt  = y_mask[:, 4:7]\n",
    "        m_tox  = y_mask[:, 7:9]\n",
    "\n",
    "        loss_phys = masked_mse(phys_out, y_phys, m_phys)\n",
    "        loss_opt  = masked_mse(opt_out,  y_opt,  m_opt)\n",
    "        loss_tox  = masked_mse(tox_out,  y_tox,  m_tox)\n",
    "        loss_arom = criterion_bce(arom_logit, y_arom)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_phys.backward(retain_graph=True)\n",
    "        loss_opt.backward(retain_graph=True)\n",
    "        loss_tox.backward(retain_graph=True)\n",
    "        loss_arom.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total += (loss_phys + loss_opt + loss_tox + loss_arom).item()\n",
    "        n += 1\n",
    "\n",
    "    return total / max(n, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loss(loader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        phys_out, opt_out, tox_out, arom_logit = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        y_reg  = data.y_reg\n",
    "        y_mask = data.y_mask\n",
    "        y_arom = data.y_arom\n",
    "\n",
    "        y_phys = y_reg[:, 0:4]\n",
    "        y_opt  = y_reg[:, 4:7]\n",
    "        y_tox  = y_reg[:, 7:9]\n",
    "\n",
    "        m_phys = y_mask[:, 0:4]\n",
    "        m_opt  = y_mask[:, 4:7]\n",
    "        m_tox  = y_mask[:, 7:9]\n",
    "\n",
    "        loss_phys = masked_mse(phys_out, y_phys, m_phys)\n",
    "        loss_opt  = masked_mse(opt_out,  y_opt,  m_opt)\n",
    "        loss_tox  = masked_mse(tox_out,  y_tox,  m_tox)\n",
    "        loss_arom = criterion_bce(arom_logit, y_arom)\n",
    "\n",
    "        total += (loss_phys + loss_opt + loss_tox + loss_arom).item()\n",
    "        n += 1\n",
    "\n",
    "    return total / max(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d23119d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1.793502, Test Loss: 1.694277\n",
      "Epoch: 002, Train Loss: 1.483105, Test Loss: 1.379466\n",
      "Epoch: 003, Train Loss: 1.376155, Test Loss: 1.225202\n",
      "Epoch: 004, Train Loss: 1.305477, Test Loss: 1.266947\n",
      "Epoch: 005, Train Loss: 1.257684, Test Loss: 1.109159\n",
      "Epoch: 006, Train Loss: 1.227248, Test Loss: 1.069350\n",
      "Epoch: 007, Train Loss: 1.193537, Test Loss: 0.978373\n",
      "Epoch: 008, Train Loss: 1.170049, Test Loss: 0.936978\n",
      "Epoch: 009, Train Loss: 1.136460, Test Loss: 1.093803\n",
      "Epoch: 010, Train Loss: 1.114572, Test Loss: 0.920366\n",
      "Epoch: 011, Train Loss: 1.108171, Test Loss: 1.707376\n",
      "Epoch: 012, Train Loss: 1.099836, Test Loss: 1.379006\n",
      "Epoch: 013, Train Loss: 1.073693, Test Loss: 0.860024\n",
      "Epoch: 014, Train Loss: 1.065489, Test Loss: 0.875813\n",
      "Epoch: 015, Train Loss: 1.057918, Test Loss: 0.865504\n",
      "Epoch: 016, Train Loss: 1.039642, Test Loss: 0.929046\n",
      "Epoch: 017, Train Loss: 1.033110, Test Loss: 0.873882\n",
      "Epoch: 018, Train Loss: 1.009664, Test Loss: 0.884544\n",
      "Epoch: 019, Train Loss: 1.005962, Test Loss: 0.847359\n",
      "Epoch: 020, Train Loss: 1.024436, Test Loss: 0.834045\n",
      "Epoch: 021, Train Loss: 0.996997, Test Loss: 0.823418\n",
      "Epoch: 022, Train Loss: 0.977773, Test Loss: 0.812044\n",
      "Epoch: 023, Train Loss: 0.958677, Test Loss: 1.095732\n",
      "Epoch: 024, Train Loss: 0.975025, Test Loss: 0.831322\n",
      "Epoch: 025, Train Loss: 0.972913, Test Loss: 1.135765\n",
      "Epoch: 026, Train Loss: 0.931762, Test Loss: 0.863813\n",
      "Epoch: 027, Train Loss: 0.936785, Test Loss: 0.862329\n",
      "Epoch: 028, Train Loss: 0.933993, Test Loss: 1.878657\n",
      "Epoch: 029, Train Loss: 0.876408, Test Loss: 0.771263\n",
      "Epoch: 030, Train Loss: 0.850365, Test Loss: 0.772980\n",
      "Epoch: 031, Train Loss: 0.824098, Test Loss: 0.757976\n",
      "Epoch: 032, Train Loss: 0.823311, Test Loss: 0.832869\n",
      "Epoch: 033, Train Loss: 0.822559, Test Loss: 0.754169\n",
      "Epoch: 034, Train Loss: 0.810725, Test Loss: 0.767110\n",
      "Epoch: 035, Train Loss: 0.812948, Test Loss: 0.827175\n",
      "Epoch: 036, Train Loss: 0.825968, Test Loss: 0.761709\n",
      "Epoch: 037, Train Loss: 0.807392, Test Loss: 0.754695\n",
      "Epoch: 038, Train Loss: 0.810425, Test Loss: 0.774514\n",
      "Epoch: 039, Train Loss: 0.806107, Test Loss: 0.781155\n",
      "Epoch: 040, Train Loss: 0.795976, Test Loss: 0.777953\n",
      "Epoch: 041, Train Loss: 0.795678, Test Loss: 0.764274\n",
      "Epoch: 042, Train Loss: 0.793511, Test Loss: 0.768773\n",
      "Epoch: 043, Train Loss: 0.796188, Test Loss: 0.774593\n",
      "Epoch: 044, Train Loss: 0.797725, Test Loss: 0.757358\n",
      "Epoch: 045, Train Loss: 0.796338, Test Loss: 0.761104\n",
      "Epoch: 046, Train Loss: 0.805272, Test Loss: 0.748723\n",
      "Epoch: 047, Train Loss: 0.801802, Test Loss: 0.759329\n",
      "Epoch: 048, Train Loss: 0.791675, Test Loss: 0.782992\n",
      "Epoch: 049, Train Loss: 0.793775, Test Loss: 0.738694\n",
      "Epoch: 050, Train Loss: 0.791138, Test Loss: 0.757818\n",
      "Epoch: 051, Train Loss: 0.799089, Test Loss: 0.748397\n",
      "Epoch: 052, Train Loss: 0.791604, Test Loss: 0.758073\n",
      "Epoch: 053, Train Loss: 0.797232, Test Loss: 0.790420\n",
      "Epoch: 054, Train Loss: 0.795027, Test Loss: 0.756607\n",
      "Epoch: 055, Train Loss: 0.808763, Test Loss: 0.834218\n",
      "Epoch: 056, Train Loss: 0.791952, Test Loss: 0.747174\n",
      "Epoch: 057, Train Loss: 0.795000, Test Loss: 0.776780\n",
      "Epoch: 058, Train Loss: 0.794978, Test Loss: 0.781807\n",
      "Epoch: 059, Train Loss: 0.792278, Test Loss: 0.767317\n",
      "Epoch: 060, Train Loss: 0.798291, Test Loss: 0.744199\n",
      "Epoch: 061, Train Loss: 0.792751, Test Loss: 0.753882\n",
      "Epoch: 062, Train Loss: 0.794513, Test Loss: 0.746164\n",
      "Epoch: 063, Train Loss: 0.807244, Test Loss: 0.756537\n",
      "Epoch: 064, Train Loss: 0.794885, Test Loss: 0.755182\n",
      "Epoch: 065, Train Loss: 0.794044, Test Loss: 0.826355\n",
      "Epoch: 066, Train Loss: 0.795567, Test Loss: 0.770171\n",
      "Epoch: 067, Train Loss: 0.793875, Test Loss: 0.791643\n",
      "Epoch: 068, Train Loss: 0.789750, Test Loss: 0.747838\n",
      "Epoch: 069, Train Loss: 0.791638, Test Loss: 0.792321\n",
      "Epoch: 070, Train Loss: 0.792334, Test Loss: 0.751026\n",
      "Epoch: 071, Train Loss: 0.809199, Test Loss: 0.760455\n",
      "Epoch: 072, Train Loss: 0.794923, Test Loss: 0.761453\n",
      "Epoch: 073, Train Loss: 0.791475, Test Loss: 0.750113\n",
      "Epoch: 074, Train Loss: 0.791533, Test Loss: 0.753752\n",
      "Epoch: 075, Train Loss: 0.792302, Test Loss: 0.762882\n",
      "Epoch: 076, Train Loss: 0.794008, Test Loss: 0.778149\n",
      "Epoch: 077, Train Loss: 0.794546, Test Loss: 0.756347\n",
      "Epoch: 078, Train Loss: 0.795724, Test Loss: 0.764696\n",
      "Epoch: 079, Train Loss: 0.791830, Test Loss: 0.870722\n",
      "Epoch: 080, Train Loss: 0.792129, Test Loss: 0.749828\n",
      "Epoch: 081, Train Loss: 0.794008, Test Loss: 0.777314\n",
      "Epoch: 082, Train Loss: 0.794964, Test Loss: 0.761845\n",
      "Epoch: 083, Train Loss: 0.799544, Test Loss: 0.751154\n",
      "Epoch: 084, Train Loss: 0.794613, Test Loss: 0.761190\n",
      "Epoch: 085, Train Loss: 0.799985, Test Loss: 0.748092\n",
      "Epoch: 086, Train Loss: 0.794787, Test Loss: 0.758347\n",
      "Epoch: 087, Train Loss: 0.793473, Test Loss: 0.743750\n",
      "Epoch: 088, Train Loss: 0.793800, Test Loss: 0.759525\n",
      "Epoch: 089, Train Loss: 0.791079, Test Loss: 0.758633\n",
      "Epoch: 090, Train Loss: 0.789870, Test Loss: 0.765212\n",
      "Epoch: 091, Train Loss: 0.792440, Test Loss: 0.756316\n",
      "Epoch: 092, Train Loss: 0.793358, Test Loss: 0.756293\n",
      "Epoch: 093, Train Loss: 0.792312, Test Loss: 0.740708\n",
      "Epoch: 094, Train Loss: 0.793000, Test Loss: 0.749599\n",
      "Epoch: 095, Train Loss: 0.794960, Test Loss: 1.049468\n",
      "Epoch: 096, Train Loss: 0.794302, Test Loss: 1.242705\n",
      "Epoch: 097, Train Loss: 0.796224, Test Loss: 0.760524\n",
      "Epoch: 098, Train Loss: 0.792523, Test Loss: 0.766587\n",
      "Epoch: 099, Train Loss: 0.791650, Test Loss: 0.754424\n",
      "Epoch: 100, Train Loss: 0.792330, Test Loss: 0.768129\n",
      "Epoch: 101, Train Loss: 0.792091, Test Loss: 0.753793\n",
      "Epoch: 102, Train Loss: 0.798066, Test Loss: 0.758966\n",
      "Epoch: 103, Train Loss: 0.793925, Test Loss: 0.756612\n",
      "Epoch: 104, Train Loss: 0.794596, Test Loss: 0.766704\n",
      "Epoch: 105, Train Loss: 0.794073, Test Loss: 0.756254\n",
      "Epoch: 106, Train Loss: 0.791587, Test Loss: 0.862625\n",
      "Epoch: 107, Train Loss: 0.799942, Test Loss: 0.757078\n",
      "Epoch: 108, Train Loss: 0.796169, Test Loss: 0.750330\n",
      "Epoch: 109, Train Loss: 0.794533, Test Loss: 0.759570\n",
      "Epoch: 110, Train Loss: 0.791960, Test Loss: 0.754278\n",
      "Epoch: 111, Train Loss: 0.792782, Test Loss: 0.750860\n",
      "Epoch: 112, Train Loss: 0.794989, Test Loss: 0.749179\n",
      "Epoch: 113, Train Loss: 0.792382, Test Loss: 0.747569\n",
      "Epoch: 114, Train Loss: 0.786881, Test Loss: 0.746580\n",
      "Epoch: 115, Train Loss: 0.793328, Test Loss: 0.819875\n",
      "Epoch: 116, Train Loss: 0.791972, Test Loss: 0.753108\n",
      "Epoch: 117, Train Loss: 0.796340, Test Loss: 0.765221\n",
      "Epoch: 118, Train Loss: 0.793070, Test Loss: 0.762829\n",
      "Epoch: 119, Train Loss: 0.791523, Test Loss: 0.756121\n",
      "Epoch: 120, Train Loss: 0.793758, Test Loss: 0.747064\n",
      "Epoch: 121, Train Loss: 0.795077, Test Loss: 0.752321\n",
      "Epoch: 122, Train Loss: 0.790284, Test Loss: 0.736230\n",
      "Epoch: 123, Train Loss: 0.797775, Test Loss: 0.779437\n",
      "Epoch: 124, Train Loss: 0.790799, Test Loss: 0.745179\n",
      "Epoch: 125, Train Loss: 0.793784, Test Loss: 0.759618\n",
      "Epoch: 126, Train Loss: 0.792064, Test Loss: 0.760410\n",
      "Epoch: 127, Train Loss: 0.794440, Test Loss: 0.814097\n",
      "Epoch: 128, Train Loss: 0.792884, Test Loss: 0.757111\n",
      "Epoch: 129, Train Loss: 0.794624, Test Loss: 0.758595\n",
      "Epoch: 130, Train Loss: 0.795473, Test Loss: 0.758814\n",
      "Epoch: 131, Train Loss: 0.791593, Test Loss: 0.772395\n",
      "Epoch: 132, Train Loss: 0.795454, Test Loss: 0.752067\n",
      "Epoch: 133, Train Loss: 0.790270, Test Loss: 0.742994\n",
      "Epoch: 134, Train Loss: 0.791233, Test Loss: 0.751176\n",
      "Epoch: 135, Train Loss: 0.791417, Test Loss: 0.759925\n",
      "Epoch: 136, Train Loss: 0.794756, Test Loss: 0.760124\n",
      "Epoch: 137, Train Loss: 0.794377, Test Loss: 1.170699\n",
      "Epoch: 138, Train Loss: 0.793354, Test Loss: 0.747177\n",
      "Epoch: 139, Train Loss: 0.793771, Test Loss: 0.754257\n",
      "Epoch: 140, Train Loss: 0.791724, Test Loss: 0.754656\n",
      "Epoch: 141, Train Loss: 0.801979, Test Loss: 0.749244\n",
      "Epoch: 142, Train Loss: 0.801532, Test Loss: 0.762167\n",
      "Epoch: 143, Train Loss: 0.793282, Test Loss: 0.749090\n",
      "Epoch: 144, Train Loss: 0.788926, Test Loss: 0.765471\n",
      "Epoch: 145, Train Loss: 0.796256, Test Loss: 0.768637\n",
      "Epoch: 146, Train Loss: 0.792757, Test Loss: 0.757668\n",
      "Epoch: 147, Train Loss: 0.793166, Test Loss: 0.763329\n",
      "Epoch: 148, Train Loss: 0.790489, Test Loss: 0.766970\n",
      "Epoch: 149, Train Loss: 0.794098, Test Loss: 0.756883\n",
      "Epoch: 150, Train Loss: 0.793839, Test Loss: 0.853908\n",
      "Epoch: 151, Train Loss: 0.792647, Test Loss: 0.769173\n",
      "Epoch: 152, Train Loss: 0.795007, Test Loss: 0.761580\n",
      "Epoch: 153, Train Loss: 0.792260, Test Loss: 0.748699\n",
      "Epoch: 154, Train Loss: 0.791600, Test Loss: 0.747606\n",
      "Epoch: 155, Train Loss: 0.793928, Test Loss: 0.765456\n",
      "Epoch: 156, Train Loss: 0.791872, Test Loss: 0.757332\n",
      "Epoch: 157, Train Loss: 0.793989, Test Loss: 0.751588\n",
      "Epoch: 158, Train Loss: 0.791972, Test Loss: 0.753581\n",
      "Epoch: 159, Train Loss: 0.791021, Test Loss: 0.749998\n",
      "Epoch: 160, Train Loss: 0.794138, Test Loss: 0.753469\n",
      "Epoch: 161, Train Loss: 0.803421, Test Loss: 0.754838\n",
      "Epoch: 162, Train Loss: 0.794495, Test Loss: 0.765282\n",
      "Epoch: 163, Train Loss: 0.790398, Test Loss: 0.760298\n",
      "Epoch: 164, Train Loss: 0.798370, Test Loss: 0.762170\n",
      "Epoch: 165, Train Loss: 0.793896, Test Loss: 0.747794\n",
      "Epoch: 166, Train Loss: 0.794249, Test Loss: 0.760341\n",
      "Epoch: 167, Train Loss: 0.795041, Test Loss: 0.770157\n",
      "Epoch: 168, Train Loss: 0.793961, Test Loss: 0.743325\n",
      "Epoch: 169, Train Loss: 0.791584, Test Loss: 0.758601\n",
      "Epoch: 170, Train Loss: 0.792029, Test Loss: 0.751377\n",
      "Epoch: 171, Train Loss: 0.796249, Test Loss: 0.760555\n",
      "Epoch: 172, Train Loss: 0.793835, Test Loss: 1.295858\n",
      "Epoch: 173, Train Loss: 0.792002, Test Loss: 0.769249\n",
      "Epoch: 174, Train Loss: 0.790272, Test Loss: 0.799705\n",
      "Epoch: 175, Train Loss: 0.793718, Test Loss: 0.767613\n",
      "Epoch: 176, Train Loss: 0.792191, Test Loss: 0.756182\n",
      "Epoch: 177, Train Loss: 0.793017, Test Loss: 0.780934\n",
      "Epoch: 178, Train Loss: 0.798131, Test Loss: 0.746542\n",
      "Epoch: 179, Train Loss: 0.793365, Test Loss: 0.870334\n",
      "Epoch: 180, Train Loss: 0.792305, Test Loss: 0.769762\n",
      "Epoch: 181, Train Loss: 0.794163, Test Loss: 0.833681\n",
      "Epoch: 182, Train Loss: 0.793966, Test Loss: 0.754776\n",
      "Epoch: 183, Train Loss: 0.798243, Test Loss: 0.789195\n",
      "Epoch: 184, Train Loss: 0.794276, Test Loss: 0.750058\n",
      "Epoch: 185, Train Loss: 0.792908, Test Loss: 0.769363\n",
      "Epoch: 186, Train Loss: 0.790852, Test Loss: 0.770034\n",
      "Epoch: 187, Train Loss: 0.792177, Test Loss: 0.762358\n",
      "Epoch: 188, Train Loss: 0.792573, Test Loss: 0.761655\n",
      "Epoch: 189, Train Loss: 0.793250, Test Loss: 0.749328\n",
      "Epoch: 190, Train Loss: 0.793308, Test Loss: 0.756548\n",
      "Epoch: 191, Train Loss: 0.798326, Test Loss: 0.755785\n",
      "Epoch: 192, Train Loss: 0.792721, Test Loss: 0.744497\n",
      "Epoch: 193, Train Loss: 0.792606, Test Loss: 0.751937\n",
      "Epoch: 194, Train Loss: 0.798656, Test Loss: 0.798249\n",
      "Epoch: 195, Train Loss: 0.800940, Test Loss: 0.767732\n",
      "Epoch: 196, Train Loss: 0.795806, Test Loss: 0.758875\n",
      "Epoch: 197, Train Loss: 0.793692, Test Loss: 0.762906\n",
      "Epoch: 198, Train Loss: 0.794970, Test Loss: 0.762050\n",
      "Epoch: 199, Train Loss: 0.799005, Test Loss: 0.757959\n",
      "Epoch: 200, Train Loss: 0.792676, Test Loss: 0.760515\n",
      "Epoch: 201, Train Loss: 0.791298, Test Loss: 0.760219\n",
      "Epoch: 202, Train Loss: 0.795902, Test Loss: 0.747045\n",
      "Epoch: 203, Train Loss: 0.793027, Test Loss: 0.752457\n",
      "Epoch: 204, Train Loss: 0.796448, Test Loss: 0.765138\n",
      "Epoch: 205, Train Loss: 0.797717, Test Loss: 0.758805\n",
      "Epoch: 206, Train Loss: 0.791661, Test Loss: 0.755725\n",
      "Epoch: 207, Train Loss: 0.796411, Test Loss: 0.745133\n",
      "Epoch: 208, Train Loss: 0.790463, Test Loss: 0.787389\n",
      "Epoch: 209, Train Loss: 0.792495, Test Loss: 0.751858\n",
      "Epoch: 210, Train Loss: 0.793214, Test Loss: 0.770781\n",
      "Epoch: 211, Train Loss: 0.792588, Test Loss: 0.789224\n",
      "Epoch: 212, Train Loss: 0.803357, Test Loss: 0.749681\n",
      "Epoch: 213, Train Loss: 0.795049, Test Loss: 0.788325\n",
      "Epoch: 214, Train Loss: 0.791206, Test Loss: 0.755345\n",
      "Epoch: 215, Train Loss: 0.792816, Test Loss: 0.791546\n",
      "Epoch: 216, Train Loss: 0.795113, Test Loss: 0.759909\n",
      "Epoch: 217, Train Loss: 0.790149, Test Loss: 0.750711\n",
      "Epoch: 218, Train Loss: 0.793069, Test Loss: 0.743313\n",
      "Epoch: 219, Train Loss: 0.798980, Test Loss: 0.752809\n",
      "Epoch: 220, Train Loss: 0.795753, Test Loss: 0.772334\n",
      "Epoch: 221, Train Loss: 0.799411, Test Loss: 0.753913\n",
      "Epoch: 222, Train Loss: 0.790690, Test Loss: 0.751415\n",
      "Epoch: 223, Train Loss: 0.797651, Test Loss: 0.758732\n",
      "Epoch: 224, Train Loss: 0.796573, Test Loss: 0.773956\n",
      "Epoch: 225, Train Loss: 0.795545, Test Loss: 0.751338\n",
      "Epoch: 226, Train Loss: 0.792429, Test Loss: 0.764794\n",
      "Epoch: 227, Train Loss: 0.793721, Test Loss: 0.755838\n",
      "Epoch: 228, Train Loss: 0.797102, Test Loss: 0.755583\n",
      "Epoch: 229, Train Loss: 0.794876, Test Loss: 0.752812\n",
      "Epoch: 230, Train Loss: 0.791917, Test Loss: 0.742595\n",
      "Epoch: 231, Train Loss: 0.793489, Test Loss: 0.752324\n",
      "Epoch: 232, Train Loss: 0.793143, Test Loss: 0.737728\n",
      "Epoch: 233, Train Loss: 0.795926, Test Loss: 0.761453\n",
      "Epoch: 234, Train Loss: 0.794726, Test Loss: 0.747599\n",
      "Epoch: 235, Train Loss: 0.791899, Test Loss: 0.751724\n",
      "Epoch: 236, Train Loss: 0.798599, Test Loss: 0.755517\n",
      "Epoch: 237, Train Loss: 0.791775, Test Loss: 0.783164\n",
      "Epoch: 238, Train Loss: 0.797369, Test Loss: 0.737712\n",
      "Epoch: 239, Train Loss: 0.793458, Test Loss: 1.045259\n",
      "Epoch: 240, Train Loss: 0.791311, Test Loss: 0.758395\n",
      "Epoch: 241, Train Loss: 0.791909, Test Loss: 0.759906\n",
      "Epoch: 242, Train Loss: 0.794346, Test Loss: 0.766792\n",
      "Epoch: 243, Train Loss: 0.792074, Test Loss: 0.751211\n",
      "Epoch: 244, Train Loss: 0.792256, Test Loss: 0.849338\n",
      "Epoch: 245, Train Loss: 0.800010, Test Loss: 0.793159\n",
      "Epoch: 246, Train Loss: 0.794577, Test Loss: 0.748900\n",
      "Epoch: 247, Train Loss: 0.795177, Test Loss: 0.760384\n",
      "Epoch: 248, Train Loss: 0.794207, Test Loss: 0.772439\n",
      "Epoch: 249, Train Loss: 0.790684, Test Loss: 0.743921\n",
      "Epoch: 250, Train Loss: 0.796669, Test Loss: 0.744103\n",
      "Epoch: 251, Train Loss: 0.791376, Test Loss: 1.268743\n",
      "Epoch: 252, Train Loss: 0.790364, Test Loss: 0.749097\n",
      "Epoch: 253, Train Loss: 0.795309, Test Loss: 0.745314\n",
      "Epoch: 254, Train Loss: 0.788600, Test Loss: 0.747990\n",
      "Epoch: 255, Train Loss: 0.793421, Test Loss: 0.746259\n",
      "Epoch: 256, Train Loss: 0.791628, Test Loss: 0.755003\n",
      "Epoch: 257, Train Loss: 0.791216, Test Loss: 0.999995\n",
      "Epoch: 258, Train Loss: 0.790734, Test Loss: 0.747661\n",
      "Epoch: 259, Train Loss: 0.796498, Test Loss: 0.774401\n",
      "Epoch: 260, Train Loss: 0.794454, Test Loss: 0.757324\n",
      "Epoch: 261, Train Loss: 0.793635, Test Loss: 0.753000\n",
      "Epoch: 262, Train Loss: 0.795228, Test Loss: 0.778522\n",
      "Epoch: 263, Train Loss: 0.790846, Test Loss: 0.745999\n",
      "Epoch: 264, Train Loss: 0.791833, Test Loss: 0.741218\n",
      "Epoch: 265, Train Loss: 0.794467, Test Loss: 0.761220\n",
      "Epoch: 266, Train Loss: 0.789922, Test Loss: 0.762019\n",
      "Epoch: 267, Train Loss: 0.790077, Test Loss: 0.777345\n",
      "Epoch: 268, Train Loss: 0.801359, Test Loss: 0.738513\n",
      "Epoch: 269, Train Loss: 0.795842, Test Loss: 0.888202\n",
      "Epoch: 270, Train Loss: 0.792075, Test Loss: 0.751772\n",
      "Epoch: 271, Train Loss: 0.794649, Test Loss: 0.747309\n",
      "Epoch: 272, Train Loss: 0.794713, Test Loss: 0.776437\n",
      "Epoch: 273, Train Loss: 0.791362, Test Loss: 0.750820\n",
      "Epoch: 274, Train Loss: 0.792628, Test Loss: 0.746016\n",
      "Epoch: 275, Train Loss: 0.797741, Test Loss: 0.756282\n",
      "Epoch: 276, Train Loss: 0.794076, Test Loss: 0.778899\n",
      "Epoch: 277, Train Loss: 0.794208, Test Loss: 0.754343\n",
      "Epoch: 278, Train Loss: 0.792411, Test Loss: 0.770278\n",
      "Epoch: 279, Train Loss: 0.800770, Test Loss: 0.759403\n",
      "Epoch: 280, Train Loss: 0.789531, Test Loss: 0.746586\n",
      "Epoch: 281, Train Loss: 0.792090, Test Loss: 0.765193\n",
      "Epoch: 282, Train Loss: 0.794362, Test Loss: 0.752411\n",
      "Epoch: 283, Train Loss: 0.790755, Test Loss: 0.768555\n",
      "Epoch: 284, Train Loss: 0.794122, Test Loss: 0.899148\n",
      "Epoch: 285, Train Loss: 0.795692, Test Loss: 0.817493\n",
      "Epoch: 286, Train Loss: 0.788429, Test Loss: 0.761867\n",
      "Epoch: 287, Train Loss: 0.795772, Test Loss: 0.753365\n",
      "Epoch: 288, Train Loss: 0.788144, Test Loss: 0.758411\n",
      "Epoch: 289, Train Loss: 0.796415, Test Loss: 0.753743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# header\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch()\n\u001b[0;32m     22\u001b[0m     test_loss  \u001b[38;5;241m=\u001b[39m eval_loss(test_loader)\n\u001b[0;32m     23\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      4\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     phys_out, opt_out, tox_out, arom_logit \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:27\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[0;32m     28\u001b[0m         batch,\n\u001b[0;32m     29\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[0;32m     30\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[0;32m     31\u001b[0m     )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m collate(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m     99\u001b[0m         data_list\u001b[38;5;241m=\u001b[39mdata_list,\n\u001b[0;32m    100\u001b[0m         increment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    101\u001b[0m         add_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_list[\u001b[38;5;241m0\u001b[39m], Batch),\n\u001b[0;32m    102\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39mfollow_batch,\n\u001b[0;32m    103\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39mexclude_keys,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[0;32m    110\u001b[0m                                increment)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:169\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[1;32m--> 169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m get_incs(key, values, data_list, stores)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    172\u001b[0m             value \u001b[38;5;241m+\u001b[39m inc\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n\u001b[0;32m    174\u001b[0m         ]\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:333\u001b[0m, in \u001b[0;36mget_incs\u001b[1;34m(key, values, data_list, stores)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(repeats)\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cumsum(repeats[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\johsnonstark\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\functions.py:24\u001b[0m, in \u001b[0;36mcumsum\u001b[1;34m(x, dim)\u001b[0m\n\u001b[0;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnew_empty(size)\n\u001b[0;32m     23\u001b[0m out\u001b[38;5;241m.\u001b[39mnarrow(dim, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m---> 24\u001b[0m torch\u001b[38;5;241m.\u001b[39mcumsum(x, dim\u001b[38;5;241m=\u001b[39mdim, out\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mnarrow(dim, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(dim)))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Train + log to CSV\n",
    "# ----------------------------\n",
    "num_epochs = 1000\n",
    "log_path = \"loss_log.csv\"\n",
    "\n",
    "best_test = math.inf\n",
    "best_epoch = -1\n",
    "best_train_loss = None\n",
    "best_test_loss = None\n",
    "best_model_state = None\n",
    "best_optimizer_state = None\n",
    "best_scheduler_state = None\n",
    "\n",
    "with open(log_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"train_loss\", \"test_loss\"])  # header\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        \n",
    "        train_loss = train_epoch()\n",
    "        test_loss  = eval_loss(test_loader)\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        writer.writerow([epoch, float(train_loss), float(test_loss)])\n",
    "        print(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "        # find best model \n",
    "        if test_loss < best_test:\n",
    "            best_test = float(test_loss)\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_train_loss = float(train_loss)\n",
    "            best_test_loss  = float(test_loss)\n",
    "\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "            best_scheduler_state = copy.deepcopy(scheduler.state_dict())\n",
    "\n",
    "print(f\"Saved: {log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503535b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Save \n",
    "# ----------------------------\n",
    "\n",
    "# LAST model\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"train_loss\": float(train_loss),\n",
    "    \"test_loss\": float(test_loss),\n",
    "    \n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    \n",
    "    \"scaler_mean\": scaler.mean_,\n",
    "    \"scaler_scale\": scaler.scale_,\n",
    "    \"reg_cols\": REG_COLS,\n",
    "    \"bin_col\": BIN_COL,  \n",
    "    }, \n",
    "    \"multitask_gin_last.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "# BEST model\n",
    "torch.save({\n",
    "    \"epoch\": best_epoch,\n",
    "    \"train_loss\": best_train_loss,\n",
    "    \"test_loss\": best_test_loss,\n",
    "    \n",
    "    \"model_state_dict\": best_model_state,\n",
    "    \"optimizer_state_dict\": best_optimizer_state,\n",
    "    \"scheduler_state_dict\": best_scheduler_state,\n",
    "    \n",
    "    \"scaler_mean\": scaler.mean_,\n",
    "    \"scaler_scale\": scaler.scale_,\n",
    "    \"reg_cols\": REG_COLS,\n",
    "    \"bin_col\": BIN_COL,\n",
    "    }, \n",
    "    \"multitask_gin_best.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load Model\n",
    "# ----------------------------\n",
    "\n",
    "# Load the best model checkpoint\n",
    "checkpoint = torch.load('multitask_gin_best.pth')\n",
    "scaler_mean = np.array(checkpoint[\"scaler_mean\"], dtype=np.float32)         # (9,)\n",
    "scaler_scale = np.array(checkpoint[\"scaler_scale\"], dtype=np.float32)       # (9,)\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = MultiTaskGIN(hidden_channels=128).to(device)\n",
    "\n",
    "# Load the model state_dict\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()                                                                # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1306b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"3000testbank.csv\")\n",
    "data_list = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    graph = smiles_to_graph(row[\"SMILES\"])\n",
    "    if graph is not None:\n",
    "        data_list.append(graph)\n",
    "\n",
    "test_dataset = GraphListDataset(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ad8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Make Prediction\n",
    "# ----------------------------\n",
    "\n",
    "# Function to compute RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Function to compute Pearson correlation\n",
    "def pearson_r(x, y):\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "    return corr_mat[0, 1]\n",
    "\n",
    "# Initialize list for predictions\n",
    "predictions = []\n",
    "\n",
    "# Inverse transform function to denormalize the predictions\n",
    "def reg_inverse_transform(z_norm_1x9: np.ndarray) -> np.ndarray:\n",
    "    # z_norm = (x - mean)/scale  => x = z_norm*scale + mean\n",
    "    return z_norm_1x9 * scaler_scale + scaler_mean\n",
    "\n",
    "# Iterate through the test dataset and make predictions\n",
    "for data in test_dataset:\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Run the model to get predictions\n",
    "    phys_out, opt_out, tox_out, arom_logit = model(data.x, data.edge_index, data.batch)\n",
    "    \n",
    "    # Concatenate predictions from each task (normalized)\n",
    "    pred_reg_norm = torch.cat([phys_out, opt_out, tox_out], dim=1).detach().cpu().numpy()  # (1, 9)\n",
    "    pred_arom = arom_logit.detach().cpu().numpy().reshape(1, -1)\n",
    "    \n",
    "    # Apply inverse transformation to get predictions in raw scale\n",
    "    pred_reg_raw = reg_inverse_transform(pred_reg_norm)  # (1, 9)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions.append(np.concatenate([pred_reg_raw, pred_arom], axis=1))\n",
    "    # predictions.append(pred_reg_raw)\n",
    "\n",
    "# Convert predictions to a numpy array for easier processing later\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e143e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load true values from CSV\n",
    "true_values = test_df[[\"Solubility\", \"Molecular_Weight\", \"Log_P\", \"BoilingPoint\", \"sigma_780nm\",\n",
    "                       \"max_sigma\", \"ISC(S1-T1)\", \"SAscore\", \"Tox_score\", \"is_aromatic\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Recording R2 & RMSE\n",
    "# ----------------------------\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Compute RMSE and Pearson correlation for each property\n",
    "for i, prop in enumerate(REG_COLS + [BIN_COL]):  # Includes both regression and arom_logit\n",
    "    y_true = true_values[:, i]\n",
    "    y_pred = predictions[:, i]\n",
    "\n",
    "    # Filter out NaN values (only valid pairs will be considered)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"{prop}: no valid pairs (all NA)\")\n",
    "        continue\n",
    "\n",
    "    # Compute RMSE\n",
    "    property_rmse = rmse(y_true, y_pred)\n",
    "\n",
    "    # Compute Pearson correlation\n",
    "    property_pearson = pearson_r(y_true, y_pred)\n",
    "\n",
    "    # Print the results for all properties, including \"is_aromatic\"\n",
    "    print(f\"{prop} RMSE: {property_rmse:.4f}\")\n",
    "    print(f\"{prop} Pearson r: {property_pearson:.4f}\")\n",
    "\n",
    "    # Append the results for this property\n",
    "    results.append([prop, property_pearson, property_rmse])\n",
    "\n",
    "# Define the header for the CSV\n",
    "header = [\"Property\", \"Pearson r\", \"RMSE\"]\n",
    "\n",
    "# Write the results to a CSV file\n",
    "output_csv = \"R2&RMSE_GIN.csv\"\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)  # Write the header\n",
    "    writer.writerows(results)  # Write the data rows\n",
    "\n",
    "print(f\"Results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189658dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Drawing R2\n",
    "# ----------------------------\n",
    "\n",
    "# Dictionary to store Pearson correlation for each property\n",
    "r_by_prop = {}\n",
    "\n",
    "# Iterate through each property\n",
    "for i, prop in enumerate(REG_COLS + [BIN_COL]):  # Includes both regression and arom_logit\n",
    "    y_true = true_values[:, i]\n",
    "    y_pred = predictions[:, i]\n",
    "\n",
    "    # Filter out NaN values (only valid pairs will be considered)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "\n",
    "    # Skip properties with no valid data points\n",
    "    if len(y_true) == 0:\n",
    "        r_by_prop[prop] = np.nan\n",
    "        print(f\"{prop}: no valid pairs (all NA)\")\n",
    "        continue\n",
    "\n",
    "    # Compute Pearson correlation\n",
    "    r = pearson_r(y_true, y_pred)\n",
    "    r_by_prop[prop] = r\n",
    "\n",
    "    # Create the scatter plot for true vs predicted values\n",
    "    plt.figure()\n",
    "    plt.ticklabel_format(axis=\"both\", style=\"sci\", scilimits=(0, 0))\n",
    "    plt.scatter(y_true, y_pred, s=5, color=\"purple\")\n",
    "\n",
    "    # Set the limits and padding for the plot\n",
    "    lo = min(y_true.min(), y_pred.min())\n",
    "    hi = max(y_true.max(), y_pred.max())\n",
    "    pad = 0.05 * (hi - lo) if hi > lo else 1.0  # Handle constant values\n",
    "\n",
    "    plt.plot([lo - pad, hi + pad], [lo - pad, hi + pad], c=\"k\")  # Line of perfect correlation\n",
    "    plt.xlim(lo - pad, hi + pad)\n",
    "    plt.ylim(lo - pad, hi + pad)\n",
    "\n",
    "    # Set the title and labels\n",
    "    plt.title(f\"{prop} (Pearson r = {r:.3f})\")\n",
    "    plt.xlabel(f\"True {prop}\")\n",
    "    plt.ylabel(f\"Pred {prop}\")\n",
    "    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    # Save the plot as a PDF\n",
    "    plt.savefig(f\"{prop}_pearsonr_plot.pdf\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    # Print the Pearson correlation for this property\n",
    "    print(f\"{prop}: Pearson r = {r:.3f}\")\n",
    "\n",
    "# Display all the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
